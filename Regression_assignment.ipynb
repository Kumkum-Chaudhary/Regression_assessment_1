{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0YFel9n1Y3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- What is Simple Linear Regression ?\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two continuous variables: one independent variable (X) and one dependent variable (Y). It aims to find the best-fitting straight line, represented by the equation Y = mX + c, which predicts the value of Y based on the value of X."
      ],
      "metadata": {
        "id": "jTzopxqI1lO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: Constant variance of errors (residuals).\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: (though not applicable in simple regression, important in multiple regression)."
      ],
      "metadata": {
        "id": "Wr2gMt7n12zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3- What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "The coefficient m is the slope of the line. It represents the change in the dependent variable (Y) for a one-unit increase in the independent variable (X). A positive m indicates a positive relationship, while a negative m suggests an inverse relationship."
      ],
      "metadata": {
        "id": "w2S0Ct6k2Wzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4- What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "The intercept c is the value of the dependent variable (Y) when the independent variable (X) is zero. It shows where the regression line crosses the Y-axis and provides context for the starting point of the prediction."
      ],
      "metadata": {
        "id": "pc6iJdJx2hqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5- How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "The slope m is calculated using the formula:\n",
        "m = Σ((X - X̄)(Y - Ȳ)) / Σ((X - X̄)²)\n",
        "This formula computes how changes in X are associated with changes in Y, by comparing their deviations from their means."
      ],
      "metadata": {
        "id": "FgITR1kD2hsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6- What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The least squares method minimizes the sum of the squares of the residuals (differences between actual and predicted values). Its goal is to find the line that best fits the data by ensuring the smallest possible error between observed and predicted values."
      ],
      "metadata": {
        "id": "k-ur1Wyy2hur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7- How is the coefficient of determination (R²) interpreted in Simple Linear\n",
        "    Regression?\n",
        "    \n",
        "R² measures how well the regression model explains the variability of the dependent variable. It ranges from 0 to 1. A value closer to 1 means the model explains most of the variation, while a value closer to 0 means poor explanatory power."
      ],
      "metadata": {
        "id": "40K1J6bC2hw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8- What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between one dependent variable and two or more independent variables. The equation is Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ, where each coefficient represents the effect of a specific predictor on Y."
      ],
      "metadata": {
        "id": "v_m-A5S-2h0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9- What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "The main difference is the number of independent variables used. Simple Linear Regression uses one independent variable, while Multiple Linear Regression involves two or more, allowing for more complex modeling of relationships."
      ],
      "metadata": {
        "id": "wJogOynV2h8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10- What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Key assumptions include:\n",
        "\n",
        "Linearity: Linear relationship between predictors and the response.\n",
        "\n",
        "Independence of errors.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normal distribution of residuals.\n",
        "\n",
        "No multicollinearity: Predictors should not be highly correlated."
      ],
      "metadata": {
        "id": "J6IK6krD5ezH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity refers to non-constant variance of residuals across values of the independent variables. It can lead to biased standard errors, which affect hypothesis testing and confidence intervals, potentially making results unreliable."
      ],
      "metadata": {
        "id": "ETsgr8j45e2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12- How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "To address multicollinearity:\n",
        "\n",
        "Remove or combine highly correlated predictors.\n",
        "\n",
        "Use dimensionality reduction techniques like PCA.\n",
        "\n",
        "Apply regularization methods like Ridge or Lasso regression to penalize large coefficients."
      ],
      "metadata": {
        "id": "vXCbQVx35e4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13- What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Common techniques include:\n",
        "\n",
        "One-hot encoding: Converts categories into binary columns.\n",
        "\n",
        "Label encoding: Assigns numerical values to categories (used cautiously).\n",
        "\n",
        "Ordinal encoding: Used when the categories have a meaningful order."
      ],
      "metadata": {
        "id": "K9qTFHIf5e6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14- What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms model the combined effect of two variables when their influence on the dependent variable depends on each other. It helps capture relationships that are not simply additive but interdependent."
      ],
      "metadata": {
        "id": "mg4DPVPf5e8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15- How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In Simple Linear Regression, the intercept is the value of Y when X is zero. In Multiple Linear Regression, it is the value of Y when all independent variables are zero, which may not always be meaningful depending on the context or data scale."
      ],
      "metadata": {
        "id": "LeolZeF15e_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16- What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope shows how much the dependent variable changes for a one-unit change in an independent variable. It’s critical for making predictions, understanding variable influence, and determining the direction (positive or negative) of the relationship."
      ],
      "metadata": {
        "id": "UO4WcC1t5fCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17- How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept gives the starting point of the model — it shows the expected value of the dependent variable when all predictors are zero. It helps anchor the regression line but should be interpreted carefully, especially when zero isn’t a realistic value for inputs."
      ],
      "metadata": {
        "id": "GXkY77lT5fIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18- What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "R² only measures how well the model fits the data but does not indicate whether the predictors are meaningful or if the model will perform well on new data. It also doesn’t penalize overfitting and can be artificially inflated by adding more variables."
      ],
      "metadata": {
        "id": "PoDTwrE75fEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19- How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error suggests that the estimate of the coefficient is not precise, possibly due to high variability, small sample size, or multicollinearity. It may indicate the coefficient is not statistically significant."
      ],
      "metadata": {
        "id": "7w_D69Ga7988"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20- How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Heteroscedasticity is identified by plotting residuals vs. predicted values. A funnel shape or pattern indicates non-constant variance. It’s important to address it because it can lead to incorrect inferences, invalid confidence intervals, and biased hypothesis tests."
      ],
      "metadata": {
        "id": "rkn3BlOj7-FU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21- What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "A high R² with a low adjusted R² suggests that although the model seems to explain a large proportion of variance in the dependent variable, some of the independent variables might be irrelevant or not useful. Adjusted R² penalizes the model for adding predictors that do not improve its performance, helping avoid overfitting."
      ],
      "metadata": {
        "id": "ksj17hkE7-H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22- Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling variables ensures that all features are on the same scale, especially when they have very different units or magnitudes. This is important because it prevents variables with larger ranges from dominating the regression model and improves numerical stability and interpretability of the coefficients."
      ],
      "metadata": {
        "id": "GNF1Syfd7-LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23- What is polynomial regression?\n",
        "\n",
        "Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial. It can capture non-linear patterns by fitting a curve to the data instead of a straight line."
      ],
      "metadata": {
        "id": "s7T5qFYa96rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24-How does polynomial regression differ from linear regression?\n",
        "\n",
        "While linear regression fits a straight line to the data, polynomial regression fits a curved line by including powers of the independent variable (e.g., x², x³). Although it’s still linear in terms of coefficients, the relationship between variables becomes non-linear."
      ],
      "metadata": {
        "id": "tSDlZvkX96uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25- When is polynomial regression used?\n",
        "\n",
        "Polynomial regression is used when the data shows a non-linear relationship that cannot be captured by a straight line. It is especially helpful in cases where the trend increases or decreases at an increasing rate or has curvature."
      ],
      "metadata": {
        "id": "7wZmK3hB96xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26- What is the general equation for polynomial regression?\n",
        "\n",
        "The general form of a polynomial regression equation of degree n is:\n",
        "Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ + ε\n",
        "where b₀ is the intercept, b₁...bₙ are the coefficients, and ε is the error term."
      ],
      "metadata": {
        "id": "6RPts5Ko966m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27- Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, polynomial regression can be extended to multiple variables by including polynomial terms of each variable (like X₁², X₂²) and interaction terms (like X₁X₂). However, the model becomes complex and prone to overfitting, especially with high-degree polynomials."
      ],
      "metadata": {
        "id": "L7iXJavB9696"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zlvyoh0i-3lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28- What are the limitations of polynomial regression?\n",
        "\n",
        "Some key limitations include:\n",
        "\n",
        "Overfitting with high-degree polynomials.\n",
        "\n",
        "Poor extrapolation beyond the range of data.\n",
        "\n",
        "Interpretability becomes difficult with many terms.\n",
        "\n",
        "Sensitive to outliers, which can distort the curve significantly."
      ],
      "metadata": {
        "id": "sWJsPdr0-3nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29- What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Common methods include:\n",
        "\n",
        "Cross-validation to check model performance on unseen data.\n",
        "\n",
        "Adjusted R², which accounts for the number of predictors.\n",
        "\n",
        "AIC/BIC (Akaike/Bayesian Information Criterion) to balance fit and complexity.\n",
        "\n",
        "Residual plots to check randomness of errors."
      ],
      "metadata": {
        "id": "gCbqDf7W_KQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30- Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization helps in understanding the model’s fit to the data. It can reveal if the curve captures the pattern well or if it overfits/underfits. Plotting actual vs. predicted values also helps detect non-linearity, outliers, or bias in predictions."
      ],
      "metadata": {
        "id": "reKtNoBm-3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#31-  How is polynomial regression implemented in Python?\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "fjFJNhXb_oL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}